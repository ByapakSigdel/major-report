\section{Background Theory}
In recent years, computer technology has evolved rapidly, becoming an integral and ever-present part of human life. Modern systems are increasingly ubiquitous, blending seamlessly into our surroundings and enabling more natural and intuitive interactions between humans and machines. Traditional interfaces like keyboards and mice are being replaced with Natural User Interfaces (NUI), which allow people to interact with systems using natural human behaviors such as speech, movement, and gestures. This aligns with the Embodied Interaction Theory, which emphasizes interaction through physical engagement and bodily movement.

A notable example of gesture-based input is the data glove, a wearable device equipped with sensors that capture precise hand and finger movements. This project explores the use of a data glove for gesture recognition and its integration into interactive systems. By translating physical hand movements into digital commands, the data glove offers a more natural, immersive, and responsive way to control applications from controlling devices to navigating virtual environments.

This project focuses on the development of a data glove using Arduino, designed to recognize hand gestures and translate them into control signals for digital systems. The glove integrates flex sensors to detect finger bending, an MPU-6050 sensor (Inertial Measurement Unit) to measure orientation and motion, and an ESP8266 Wi-Fi module for wireless communication with other devices or systems. The glove structure is built using PLA (Polylactic Acid), a biodegradable 3D printing material, making the design both lightweight and customizable.

\subsection{Applications}
The data glove system finds applications in various fields including:
\begin{itemize}
    \item Virtual and Augmented Reality (VAR)
    \item Assistive Technology
    \item Gaming
    \item Robotics Control
    \item Smart Environments / IoT Applications
    \item Educational Tools
\end{itemize}

\subsection{Scope}
The project encompasses several key areas for development and future expansion:
\begin{itemize}
    \item Integration with machine learning models to improve gesture recognition accuracy
    \item Addition of haptic feedback for more immersive interaction
    \item Expansion to full-body motion capture using additional wearable sensors
    \item Development of a mobile or desktop interface for visualizing and mapping gestures
    \item Incorporating voice + gesture multimodal control systems
\end{itemize}
